<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[numpy库的使用]]></title>
    <url>%2F2019%2F08%2F16%2Fnumpy%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言numpy库是python中一个非常重要的库,它提供了一个多维数组（ndarray）数据类型，以及关于多维数组的操作，NumPy 已经成为其他大数据和机器学习模块的基础。如果不想安装numpy库的,而你的电脑里没有安装python语言的,我们可以使用Anaconda,Anaconda里包含了很多我们要用到的科学包,包括了panda,numpy等,Anaconda可以在它的官网下载,速度可能会较慢. 官网地址为:https://www.anaconda.com/ 安装Anaconda的教程可以看这篇文章:https://blog.csdn.net/qq_39610888/article/details/80805356 如果想看视频的可以去b站看这个视频:https://www.bilibili.com/video/av23124018?from=search&amp;seid=17113017816135374394 安装完,我们就能在anaconda自带的juptyer里写代码了. numpy的数组属性在使用numpy我们需要像java一样,先进行导包 1import numpy as np numpy我们数组创建的常规创建方法 12345array=np.array([[1,2,3], [4,5,6], [7,8,9]])print(array) 输出结果 123[[1 2 3] [4 5 6] [7 8 9]] 这里我们的第一个array数组就创建好了. 我们可以使用numpy里的基本属性来获取我们新建数组的一些信息 ndarray.ndimNumPy 数组的维数称为秩（rank），一维数组的秩为 1，二维数组的秩为 2，以此类推。 1print(array.ndim) ndarray.ndim 用于返回数组的维数，等于秩。 输出结果为： 12 ndarray.shape表示数组的维度，返回一个元组，这个元组的长度就是维度的数目，即 ndim 属性(秩)。比如，一个二维数组，其维度表示”行数”和”列数”。 ndarray.shape 也可以用于调整数组大小。 1print(array.shape)#代表矩阵的维度 输出结果为： 1(3, 3) ndarray.size数组元素的总个数，相当于 .shape 中 n*m 的值 1print(array.size)#代表数组元素的总个数 输出结果为： 19 ndarray.dtypendarray 对象的元素类型 1print(array.dtype)#代表矩阵中元素的属性 输出结果为： 1int32 numpy的数据类型numpy 支持的数据类型比 Python 内置的类型要多很多，基本上可以和 C 语言的数据类型对应上，其中部分类型对应为 Python 内置的类型。下表列举了常用 NumPy 基本类型。 创建array的方法numpy.zeros创建指定大小的数组，数组元素以 0 来填充： 123import numpy as npzero=np.zeros((2,3))#生成2行3列全为0的矩阵print(zero) 输出结果为： 12[[0. 0. 0.] [0. 0. 0.]] numpy.ones创建指定形状的数组，数组元素以 1 来填充： 123import numpy as npone=np.ones((3,4))#生成3行4列全为1的矩阵print(one) 输出结果为： 123[[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] numpy.emptynumpy.empty 方法用来创建一个指定形状（shape）、数据类型（dtype）且未初始化的数组： 123import numpy as npempty=np.empty((3,2))#生成3行2列全都接近0（不等于0）的矩阵print(empty) 输出结果为： 123[[0. 0.] [0. 0.] [0. 0.]] numpy.arangenumpy 包中的使用 arange 函数创建数值范围并返回 ndarray 对象，函数格式如下： 1numpy.arange(start, stop, step, dtype) 根据 start 与 stop 指定的范围以及 step 设定的步长，生成一个 ndarray。 参数说明： 参数 描述 start 起始值，默认为0 stop 终止值（不包含） step 步长，默认为1 dtype 返回ndarray的数据类型，如果没有提供，则会使用输入数据的类型。 实例 生成 0 到 9 的数组: 123import numpy as npe=np.arange(10)print(e) 输出结果如下： 1[0 1 2 3 4 5 6 7 8 9] 生成 4到 11 的数组: 123import numpy as npf=np.arange(4,12)print(f) 输出结果如下： 1[ 4 5 6 7 8 9 10 11] 生成 1到 19 (每生成一个数字间隔3个数字)的数组: 123import numpy as npg=np.arange(1,20,3)print(g) 输出结果如下： 1[ 1 4 7 10 13 16 19] Numpy 数组操作修改数组形状numpy.reshapenumpy.reshape 函数可以在不改变数据的条件下修改形状，格式如下： numpy.reshape(arr, newshape, order=’C’) arr：要修改形状的数组 newshape：整数或者整数数组，新的形状应当兼容原有形状 order：’C’ – 按行，’F’ – 按列，’A’ – 原顺序，’k’ – 元素在内存中的出现顺序。 123import numpy as nph=np.arange(8).reshape(4,2)#重新定义矩阵的形状print(h) 输出结果如下： 1234[[0 1] [2 3] [4 5] [6 7]] 翻转数组numpy.transposenumpy.transpose 函数用于对换数组的维度，格式如下： 1numpy.transpose(arr, axes) arr：要操作的数组 axes：整数列表，对应维度，通常所有维度都会对换。 实例 12345678910import numpy as np a = np.arange(12).reshape(3,4) print ('原数组：')print (a )print ('\n') print ('对换数组：')print (np.transpose(a)) 输出结果如下： 1234567891011原数组：[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]对换数组：[[ 0 4 8] [ 1 5 9] [ 2 6 10] [ 3 7 11]] ndarray.T一维数组不能转置 1234import numpy as nparr1=np.array([1,2,3])arr1.Tprint(arr1)#一维的array不能转置 输出结果如下： 1[1 2 3] numpy.ndarray.T 类似 numpy.transpose 12345678910import numpy as np a = np.arange(12).reshape(3,4) print (&apos;原数组：&apos;)print (a)print (&apos;\n&apos;) print (&apos;转置数组：&apos;)print (a.T) 输出结果如下： 1234567891011原数组：[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]]转置数组：[[ 0 4 8] [ 1 5 9] [ 2 6 10] [ 3 7 11]] 改变维度使用np.newaxis()函数 例子: 12345import numpy as nparr1=np.array([1,2,3])arr_1=arr1[np.newaxis,:]#改变维度,在行这里添加一个维度，使其变成1行3列print(arr_1)print(arr_1.shape) 输出结果如下： 12[[1 2 3]](1, 3) 12345import numpy as nparr1=np.array([1,2,3])arr_2=arr1[:,np.newaxis]#在列这里添加一个维度，使其变成3行1列print(arr_2)print(arr_2.shape) 输出结果如下： 1234[[1] [2] [3]](3, 1) 使用 例子：np.atleast_2d()和np.atleast_3d()函数 123456789print(arr1)#原数据，arr1是一维数据arr_31=np.atleast_2d(arr1)#把arr1变成二维数据arr_32=np.atleast_3d(arr1)#把arr1变成三维数据print(arr_31)print(arr_32)print(arr_31.T)#二维数据转置print(arr_32.T)#三维数据转置 输出结果如下： 123456789101112131415[1 2 3][[1 2 3]][[[1] [2] [3]]] [[1] [2] [3]] [[[1] [2] [3]]] numpy的基本运算先创建数组 1234567import numpy as nparr1=np.array([[1,2,3], [4,5,6]])arr2=np.array([[1,1,2], [2,3,3]])print(arr1)print(arr2) 输出结果如下： 1234[[1 2 3] [4 5 6]][[1 1 2] [2 3 3]] 加法1print(arr1+arr2) 输出结果如下： 12[[2 3 5] [6 8 9]] 减法1print(arr1-arr2) 输出结果如下： 12[[0 1 1] [2 2 3]] 乘法1print(arr1*arr2) 输出结果如下： 12[[ 1 2 6] [ 8 15 18]] 次方1print(arr1**arr2) 输出结果如下： 12[[ 1 2 9] [ 16 125 216]] 相除1print(arr1/arr2) 输出结果如下： 12[[1. 2. 1.5 ] [2. 1.66666667 2. ]] 求余1print(arr1%arr2) 输出结果如下： 12[[0 0 1] [0 2 0]] 整除1print(arr1//arr2) 输出结果如下： 12[[1 2 1] [2 1 2]] 整体加1print(arr1+2)#所有的元素都加2 输出结果如下： 12[[3 4 5] [6 7 8]] 整体乘1print(arr1*10)#所有的元素都乘以10 输出结果如下： 12[[10 20 30] [40 50 60]] 判断大小 12arr3=arr1&gt;3#判断那个元素大于3print(arr3) 输出结果如下： 12[[False False False] [ True True True]] numpy.dot()numpy.dot() 对于两个一维的数组，计算的是这两个数组对应下标元素的乘积和(数学上称之为内积)；对于二维数组，计算的是两个数组的矩阵乘积；对于多维数组，它的通用计算公式如下，即结果数组中的每个元素都是：数组a的最后一维上的所有元素与数组b的倒数第二位上的所有元素的乘积和： dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])。 先定义一个arr4 123import numpy as nparr4=np.ones((3,5))print(arr4) 输出结果如下： 123[[1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.]] 1np.dot(arr1,arr4)#矩阵乘法 输出结果如下： 12array([[ 6., 6., 6., 6., 6.], [15., 15., 15., 15., 15.]]) 对于矩阵乘法忘记了的或者了解不深的可以看这篇文章：https://www.cnblogs.com/alantu2018/p/8528299.html 随机数生成及矩阵的运算np.random.random()函数123import numpy as npsamolel=np.random.random((3,2))#生成3行2列的随机数，范围（0-1）print(samolel) 输出结果如下： 123[[0.6150121 0.59427834] [0.58806337 0.892053 ] [0.59315305 0.92415359]] numpy.random.standard_normal()函数用法123import numpy as npsamole2=np.random.normal(size=(3,2))#生成3行2列的随机数，符合标准正态分布print(samole2) 输出结果如下： 123[[ 0.36192863 -0.30079782] [ 0.21587959 -0.15799396] [-0.87615964 1.23683354]] 矩阵的运算对于矩阵进行求和1np.sum(samolel)#对于samolel中的元素进行求和 输出结果如下： 14.206713449491107 对矩阵中的列进行求和 1np.sum(samolel,axis=0)#当axis=0时是对于列，axis=1对于行，在这里是对列求和 输出结果如下： 1array([1.79622852, 2.41048493]) 对矩阵中的列进行行求和 1np.sum(samolel,axis=1)#在这里是对行求和 输出结果如下： 1array([1.20929044, 1.48011637, 1.51730664]) 提取矩阵中的最小值1np.min(samolel)#提取samolel中的最小值 输出结果如下： 10.5880633706006644 提取矩阵中的最大值1np.max(samolel)#提取samolel中的最大值 输出结果如下： 10.9241535872093841 求矩阵中的平均值12print(np.mean(samolel))#求所有元素的平均值print(samolel.mean()) 输出结果如下： 120.70111890824851790.7011189082485179 求矩阵中的中位数1np.median(samolel)#求所有元素的中位数 输出结果如下： 10.6046452201028809 对于矩阵的数组就行开方1np.sqrt(samolel)#开方 输出结果如下： 123array([[0.78422707, 0.77089451], [0.7668529 , 0.94448558], [0.7701643 , 0.96132907]]) 对于矩阵的进行排序1np.sort(samolel)#排序 输出结果如下： 123array([[0.39407608, 0.72871605], [0.48484466, 0.73652244], [0.19709554, 0.44424115]]) 生成1行10列从1到10的随机数123import numpy as npsamole4=np.random.randint(0,10,size=(1,10))print(samole4) 输出结果如下： 1[[4 1 3 1 3 9 8 2 3 0]] 对于samole4使用np.clip()函数 1np.clip(samole4,2,7)#小于2的元素变成2，大于7的元素变成7 输出结果如下： 1array([[7, 7, 2, 6, 2, 5, 2, 5, 7, 6]]) np.clip()函数numpy.clip(a, a_min, a_max, out=None) 参数说明 a : 输入的数组 a_min: 限定的最小值 也可以是数组 如果为数组时 shape必须和a一样 a_max:限定的最大值 也可以是数组 shape和a一样 out：剪裁后的数组存入的数组 numpy的索引和切片，合并索引一维数组的索引先新建一个array数组 123import numpy as nparr1=np.arange(2,14)print(arr1) 输出结果如下： 1[ 2 3 4 5 6 7 8 9 10 11 12 13] 获取位置在2的数据1print(arr1[2])#第二个位置的数据 输出结果如下： 14 获取第一到第三个位置的数据1print(arr1[1:4])#第一到第三个位置的数据，【1:4】不会包括4 输出结果如下： 1[3 4 5] 获取第二个到倒数第一个位置的数据1print(arr1[2:-1])#第二到倒数第一个位置的数据 输出结果如下： 1[ 4 5 6 7 8 9 10 11 12] 获取前5个数据 1print(arr1[:5])#前5个数据 输出结果如下： 1[2 3 4 5 6] 获取最后两个数据1print(arr1[-2:])#取最后两个数据 输出结果如下： 1[12 13] 二维数组的索引把之前的一维数组变为二维数组 12arr2=arr1.reshape(3,4)print(arr2) 输出结果如下： 123[[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13]] 获取第一行数据对于一维数组而言,索引[1]是获取第一位的数据,而对于二维数组索引[1]是获取第一行的数据 1print(arr2[1])#第一行 输出结果如下： 1[6 7 8 9] 想要获取一个数据的话就要写清在二维数组的某一行某一列 例如: 获取第一行第一列的数据 1print(arr2[[1][1])#第一行的第一列 输出结果如下： 17 获取第一行第二列的数据 1print(arr2[1,2])#第一行的第二列 输出结果如下： 18 获取所有行的第二列 1print(arr2[:,2])#所有行的第二列 输出结果如下： 1[ 4 8 12] 切片新建一个数组 123import numpy as nparr1=np.arange(12).reshape(3,4)print(arr1) 输出结果如下： 123[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 按列切片123arr2,arr3=np.split(arr1,2,axis=1)#按列方向分割,分成2份print(arr2)print(arr3) 输出结果如下： 123456[[0 1] [4 5] [8 9]][[ 2 3] [ 6 7] [10 11]] 按行切片1234arr4,arr5,arr6=np.split(arr1,3,axis=0)#按行方向分割,分成3份print(arr4)print(arr5)print(arr6) 输出结果如下： 123[[0 1 2 3]][[4 5 6 7]][[ 8 9 10 11]] 可能报错的地方但是如果我们本身的数组,无法在我们的要求下进行切片会报错 例如： 1234arr2,arr3,arr4=np.split(arr1,3,axis=1)#按列方向分割,分成3份print(arr2)print(arr3)print(arr4) 输出结果如下： 1234567891011121314151617181920212223242526---------------------------------------------------------------------------TypeError Traceback (most recent call last)C:\ProgramData\Anaconda3\lib\site-packages\numpy\lib\shape_base.py in split(ary, indices_or_sections, axis) 842 try:--&gt; 843 len(indices_or_sections) 844 except TypeError:TypeError: object of type &apos;int&apos; has no len()During handling of the above exception, another exception occurred:ValueError Traceback (most recent call last)&lt;ipython-input-6-62c15d9e9149&gt; in &lt;module&gt;----&gt; 1 arr2,arr3,arr4=np.split(arr1,3,axis=1)#按列方向分割,分成2份 2 print(arr2) 3 print(arr3) 4 print(arr4)C:\ProgramData\Anaconda3\lib\site-packages\numpy\lib\shape_base.py in split(ary, indices_or_sections, axis) 847 if N % sections: 848 raise ValueError(--&gt; 849 &apos;array split does not result in an equal division&apos;) 850 res = array_split(ary, indices_or_sections, axis) 851 return resValueError: array split does not result in an equal division 这就是本身数组无法被切分为3份，所以报错了 而我们一定要把arr1切分为3份的话，我们可以使用np.array_split()函数 1234arr7,arr8,arr9=np.array_split(arr1,3,axis=1)#按列分割,分成3份,不等分print(arr7)print(arr8)print(arr9) 输出结果如下： 123456789[[0 1] [4 5] [8 9]][[ 2] [ 6] [10]][[ 3] [ 7] [11]] 这样arr1会被强制切分为3份 其他切分方法vsplit和hsplit1234arrv1,arrv2,arrv3=np.vsplit(arr1,3)#按行分割print(arrv1)print(arrv2)print(arrv3) 输出结果如下： 123[[0 1 2 3]][[4 5 6 7]][[ 8 9 10 11]] 123arrh1,arrh2=np.hsplit(arr1,2)#按列分割print(arrh1)print(arrh2) 输出结果如下： 123456[[0 1] [4 5] [8 9]][[ 2 3] [ 6 7] [10 11]] 合并先新建数组 行合并123456import numpy as nparr1=np.array([1,2,3])arr2=np.array([4,5,6])arr3=np.vstack((arr1,arr2))#行合并print(arr3)print(arr3.shape) 输出结果如下： 123[[1 2 3] [4 5 6]](2, 3) 列合并123arr4=np.hstack((arr1,arr2))#列合并print(arr4)print(arr4.shape) 输出结果如下： 12[1 2 3 4 5 6](6,) 也可以使用concatenate()函数合并 12arr=np.concatenate((arr1,arr2,arr1))print(arr) 输出结果如下： 1[1 2 3 4 5 6 1 2 3] 但是注意使用concatenate()函数合并，array数组的维度要相同，array的形状要匹配 numpy数组的迭代新建一个数组 123import numpy as nparr=np.random.randint(0,20,size=(3,4))print(arr) 输出结果如下： 123[[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13]] 对于arr数组进行迭代行 12for i in arr:#迭代行 print(i) 输出结果如下： 123[2 3 4 5][6 7 8 9][10 11 12 13] 对于arr数组进行迭代列 12for i in arr.T:#使用转置，来迭代列 print(i) 输出结果如下： 1234[ 2 6 10][ 3 7 11][ 4 8 12][ 5 9 13] 对于arr数组一个一个元素进行迭代 12for i in arr2.flat:#一个一个元素的迭代 print(i) 输出结果如下： 1234567891011122345678910111213 numpy的浅拷贝和深拷贝直接上例子 123456import numpy as npa=np.array([1,2,3])b=arr1 #arr1和arr2共享一块内存,浅拷贝b[0]=5print(a)print(b) 输出结果如下： 12[5 2 3][5 2 3] 可以看到，改变a后，b的值也跟着变了，这是为什么呢？ 实际上，变量a中并没有存储任何的值，它只是指向了一个内存地址，而这个地址里存储着array具体的内容，当把a赋值给b的时候，实际上是把a指向内存中某对象的链接赋给了b，也就是说，现在a和b都指向了同一个对象。 因此，在改变了内存中array的值后，而a与b都引用了该array对象，所以都一起发生了变化 这种将内存引用赋值给另一个变量的操作叫做浅拷贝 12345c=a.copy()#深拷贝c[0]=10print(a)print(c) 输出结果如下： 1234[5 2 3][10 2 3]a引用的对象的地址： 2130290221504c引用的对象的地址： 2130290057088 深拷贝呢，其实就是在赋值的时候，不把同一个内存对象的引用赋值给另一个变量，令两个变量所指向的对象不一样，更改值的时候不相互影响，这种操作就是深拷贝 copy()会创建a的一个副本，也就是创建一个一模一样的array对象，存储到内存的另一个地址中，然后将这个副本的地址赋值给c]]></content>
      <categories>
        <category>python学习记录</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用的hadoop相关安装包]]></title>
    <url>%2F2019%2F08%2F14%2F%E4%BD%BF%E7%94%A8%E7%9A%84hadoop%E7%9B%B8%E5%85%B3%E5%AE%89%E8%A3%85%E5%8C%85%2F</url>
    <content type="text"><![CDATA[前言这里放一些之前安装用到的安装包 链接：https://pan.baidu.com/s/1_qp_AKhRJNvzr6P63nJ9Rw提取码：7s9n hadoop spark hive scala 2.7.2 spark-2.4.0 hive-1.1.0 scala-2.11.4]]></content>
      <categories>
        <category>相关安装包</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala的安装]]></title>
    <url>%2F2019%2F08%2F13%2Fscala%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[前言之前写的spark的安装中忘记说明安装spark要先安装scala，因为spark的底层是使用scala脚本语言开发 使用版本 scala-2.11.4 安装spark上传解压sparkspark的压缩包上传到我们的/opt/software上，解压到/opt/module/ 代码如下 1tar -zxf scala-2.11.4.tgz -C /opt/module/ 配置环境变量1vim /etc/profile 在末尾添加 123#scalaexport SCALA_HOME=/opt/module/scala-2.11.4export PATH=$PATH:$SCALA_HOME/bin 记得保存使其生效 1source /etc/profile 验证输入 1scala -version 出现 1Scala code runner version 2.11.4 -- Copyright 2002-2013, LAMP/EPFL 则代表安装完成 之后在其他两个节点中重复上述步骤！ 安装scala到这里也就完成，一定要安装完scala后去安装spark，谢谢大家的阅读。]]></content>
      <categories>
        <category>搭建自己的hadoop学习集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark安装]]></title>
    <url>%2F2019%2F08%2F13%2Fspark%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[前言——spark介绍Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎，是类似于Hadoop MapReduce的通用并行框架。Spark拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是——Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。Spark实际上是对Hadoop的一种补充，可以很好的在Hadoop 文件系统中并行运行。 安装spark 安装版本 spark-2.4.0 上传解压spark我们将spark上传到/opt/software下，之后将其解压到/opt/module/ 解压代码如下 1tar -zxf spark-2.4.0-bin-hadoop2.7.tgz -C /opt/module/ 修改名称 1mv spark-2.4.0-bin-hadoop2.7 spark-2.4.0 配置环境变量在/etc/profile文件的最后添加 12export SPARK_HOME=/opt/module/spark-2.4.0export PATH=$PATH:$SPARK_HOME/bin 记得保存 1source /etc/profile 修改spark-env.sh文件进入spark文件夹下的conf文件夹，修改文件名 1mv spark-env.sh.template spark-env.sh 改完之后 1vim spark-env.sh 在spark-env.sh文件的末尾添加 1234export JAVA_HOME=/opt/module/jdk1.8.0_191export SCALA_HOME=/opt/module/scala-2.11.4export HADOOP_HOME=/opt/module/hadoop-2.7.2export HADOOP_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop 修改 slaves 修改 slaves 文件： 1mv slaves.template slaves 打开 slaves 文件： 1vim slaves 添加以下内容： 123hadoop102hadoop103hadoop104 完成后，我们将spark文件夹传给slave1和slave2 输入（传输的时候要在spark-2.4.0的上一目录下传输） 12scp -r spark-2.4.0 root@salve1:/opt/module/scp -r spark-2.4.0 root@salve2:/opt/module/ 传输完成，在slave1和slave2中配置环境变量 启动spark在spark-2.4.0目录下输入 1sbin/start-all.sh 启动完成后，如果可以访问 http://192.168.128.172:8080/ 如图 则为成功 到这里spark的安装也就完成了，谢谢大家的阅读。]]></content>
      <categories>
        <category>搭建自己的hadoop学习集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive的安装]]></title>
    <url>%2F2019%2F08%2F13%2Fhive%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[前言——hive介绍Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过和SQL类似的HiveQL语言快速实现简单的MapReduce统计,不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。 Hive 没有专门的数据格式。所有Hive 的数据都存储在Hadoop兼容的文件系统（例如HDFS）中。Hive 在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive 设定的目录下，因此，Hive 不支持对数据的改写和添加，所有的数据都是在加载的时候确定的。 附一张hadoop的生态圈图 安装hivehive的安装基于之前的搭建的hadoop完全分布式集群，只需要安装在hadoop102上就行 安装hive版本 hive-1.1.0 上传解压hive我们将hive上传到/opt/software下，之后将其解压到/opt/module/ 解压代码如下 1tar -zxf apache-hive-1.1.0-bin.tar.gz -C /opt/module/ 解压完成后我们修改hive的文件名 1mv apache-hive-1.1.0-bin.tar.gz hive-1.1.0 修改文件名是为了让我们在配置环境变量时更加的方便 配置环境变量在/etc/profile文件的最后添加 12export HIVE_HOME=/opt/module/hive-1.1.0export PATH=$PATH:$HIVE_HOME/sbin 配置完成后，记得保存使其生效 输入 1source /etc/profile 安装mysqlhive它有自己的内置数据库derby，但是hive 使用derby 数据库存在不支持多个连接的问题，所以我们一般会使用mysql来代替hive的元数据库 123456789101112131415161718192021[root@hadoop102 ~]# cd /opt/module/src/[root@hadoop102 src]# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm[root@hadoop102 src]# rpm -ivh mysql-community-release-el7-5.noarch.rpm[root@hadoop102 src]# yum install mysql-community-server# 这里时间较长，耐心等待...# 安装完成后，重启服务[root@hadoop102 src]# service mysqld restart[root@hadoop102 src]# mysqlWelcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 3Server version: 5.6.42 MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt;# mysql安装成功 同时我们还需要去网上下载mysql的驱动包（mysql-connector-java.jar）把这个驱动包放置在hive目录下的lib目录下。 修改hive-site.xmlhive的配置文件放置在/opt/module/hive-1.1.0/conf下 配置hive-site.xml(conf中可能没有这个文件，我们使用vim打开时，没有的话，vim会帮我们自动创建) 1vim hive-site.xml 在文件中添加 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;hive.metastore.local&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;hivepwd&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 修改hive-env.sh1[root@hadoop102 conf]# mv hive-env.sh.template hive-env.sh 在文件最后添加 12export JAVA_HOME=/opt/module/jdk1.8.0_191export HADOOP_HOME=/opt/module/hadoop-2.7.2 配置mysql 创建数据库 hive ，用来保存 Hive 元数据： 1create database hive; 同时使 root 用户可以操作数据库 hive 中的所有表： 1GRANT all ON hive.* TO root@&apos;Hadoop102&apos; IDENTIFIED BY &apos;hivepwd&apos;; 1flush privileges; 这样Hive 的元数据库就安装完成。 测试hive安装是否成功启动Hadoop与mysql 输入hive 进入hive，出现命令行就说明之前搭建是成功的 12[root@Hadoop102 ]# hivehive&gt; 测试进入Hive 命令行 执行命令 创建一个名为 test 的表 查询该表的记录数： 12create table test(id int);select count(*) from test; 如果查询结果为0，则成功 到这里，hive的安装也就完成了，谢谢大家的阅读。]]></content>
      <categories>
        <category>搭建自己的hadoop学习集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop完全分布式安装]]></title>
    <url>%2F2019%2F08%2F10%2Fhadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[前言我们之前搭建了hadoop的伪分布式的集群，伪分布式的集群对于学习一些基础的hadoop操作是足够的，但是当你涉及到更复杂的操作时，伪分布式就不够了，完全分布式的性能比伪分布的要强，而且完全分布式可操作性也更高，与此同时完全分布式对于电脑的配置也要求更高，完全分布式一般需要3台虚拟机来完成,下面我们开始搭建自己的完全分布式。 使用环境下面是本次搭建使用的环境 操作环境 主机名 IP地址 jdk hadoop版本 centos6.8 hadoop102 192.168.128.172 jdk1.8.0_191 hadoop-2.7.2 hadoop103 192.168.128.173 hadoop104 192.168.128.174 ​ 本文中使用的各种包，后续我会进行上传，以方便读者的使用 搭建步骤详解1.修改各节点的网络配置在虚拟机中输入 1vim /etc/sysconfig/network-scripts/ifcfg-eth0 可以进入虚拟机的网卡配置 我们需要修改虚拟机的网卡默认配置，将我们的虚拟机的网卡配置设置为静态ip ip地址 根据 VMware 虚拟网络进行相关配置 如图 在虚拟机里修改配置可以如图所示 修改完输入 1service network restart 重启网络服务 对其他两个hadoop节点也同样做上述操作，只不过在IPADDR值不一样，分别填其节点对应的ip 2.修改节点主机名，并且添加各节点映射在命令行中输入 1vim /etc//sysconfig/network 进入文件中修改hostname名称，如图所示 在其他两个子节点的hostname处分别填hadoop103和hadoop104 添加节点映射，输入 1vim /etc/hosts 添加节点映射为 我这里是多写了一些，对于这次搭建我们只需要添加hadoop102，hadoop103，hadoop104的节点映射即可 3.关闭防火墙我们只有关闭防火墙后才能在三台机器之间互相通信 所以关闭防火墙是很有必要的 我们可以使用这条命令来检查我们虚拟机开机时的防火墙状态 1chkconfig iptables --list 如果是已经关闭应该会如下图所示 如果没有关闭我们可以使用这两条命令来关闭我们的防火墙 1service iptables stop 这条命令是在本次虚拟机开启过程中关闭防火墙，也就是一次性关闭 我们还需要这条命令 1chkconfig iptables off 禁止防火墙关机自启动，这样防火墙就是是关闭了 当hadoop102关闭防火墙后，对于hadoop103与hadoop104也要做同样的操作 在防火墙关闭完成后，输入 1reboot 重启虚拟机，检查防火墙是否已经关闭 4.配置节点间ssh免密登陆在hadoop102上输入 1ssh-keygen -t rsa 一直按回车 完成后在保证三台虚拟机开启且完成之前所有配置的情况下输入 123[root@hadoop102 ~]# ssh-copy-id hadoop102[root@hadoop102 ~]# ssh-copy-id hadoop103[root@hadoop102 ~]# ssh-copy-id hadoop104 在hadoop102上完成后，在其他两个节点上重复上述操作 验证ssh免密登录是否成功 这里可以看到我们可以自己使用ssh转到hadoop103这台机器上 5.安装java和hadoop我们先使用xftp将hadoop和java的压缩包上传到我们新建的/opt/software上同时新建一个module文件夹放置解压后的hadoop和java，新建文件夹代码如下 123[root@hadoop102 ~]# cd /opt/[root@hadoop102 ~]# mkdir software[root@hadoop102 ~]# mkdir module 上传完成之后我们需要解压java和hadoop到/opt/module下，以便未来的管理 解压代码如下 12tar -zxf jdk-8u191-linux-x64.tar.gz -C /opt/module/tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/ 解压完成后在/opt/module下应该是这样的，如图所示 之后我们就需要配置环境变量 1vi /etc/profile 在最后添加 123456export JAVA_HOME=/opt/module/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin 退出后，输入 1source /etc/profile 使其生效 验证java和hadoop环境变量是否配置完成 123456789101112[root@hadoop103 module]# java -versionjava version &quot;1.8.0_191&quot;Java(TM) SE Runtime Environment (build 1.8.0_191-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)[root@hadoop103 module]# hadoop versionHadoop 2.7.2Subversion Unknown -r UnknownCompiled by root on 2017-05-22T10:49ZCompiled with protoc 2.5.0From source with checksum d0fda26633fa762bff87ec759ebe689cThis command was run using /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar[root@hadoop103 module]# 环境变量已经配置完成， 在其他两个节点上重复上述操作 6.配置hadoop中的文件6.1配置文件core-site.xmlcore-site.xml文件包含了NameNode主机地址，监听端口等信息，对于这个伪分布式模型来说，我的主机地址为hadoo101，NameNode默认使用的端口为8020。 修改core-site.xml 123456789101112&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop101:8020&lt;/value&gt;&lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6.2配置文件hdfs-site.xmlhdfs-site.xml用于配置/HDFS的相关属性，例如数据块的副本参数，数据块的副本对于完全分布式来说应该为3 12345678910111213&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; 6.3配置文件slavesslaves文件里面记录的是集群里所有DataNode的主机名 1234[root@hadoop102 ~]#vim slaveshadoop102hadoop103hadoop104 6.4配置文件yarn-site.xml1234567891011121314&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 6.5配置文件yarn-env.sh在其中修改java的路径 1export JAVA_HOME=/opt/module/jdk1.8.0_191 6.6配置hadoop-env.shhadoop-env.sh 由于Hadoop是java进程，所以需要添加jdk 修改hadoop-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_191 6.7配置文件mapred-site.xml先改名，因为本身是没有mapred-site.xml这个文件的 输入 1mv mapred-site.xml.template mapred-site.xml 改名完成后 123456789[root@hadoop102 ~]#vim mapred-site.xml&lt;configuration&gt;&lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 这些配置文件改好后，返回/opt/module目录下 把hadoop102下修改的文件分发到hadoop103和hadoop104下 12[root@hadoop102 module]# scp -r hadoop root@hadoop103:/opt/module/[root@hadoop102 module]# scp -r hadoop root@hadoop104:/opt/module/ 7.测试集群在完成配置文件等一系列工作后，我们要开始测试集群了 先格式化 1[root@hadoop2 hadoop-2.7.2]# bin/hdfs namenode –format 之后启动hdfs 123456789101112131415[root@hadoop2 hadoop-2.7.2]# sbin/start-dfs.sh[root@hadoop2 hadoop-2.7.2]# jps4166 NameNode4482 Jps4263 DataNode[root@hadoop3 桌面]# jps3218 DataNode3288 Jps[root@hadoop4 桌面]# jps3221 DataNode3283 SecondaryNameNode3364 Jps 如果是这样这表示启动hdfs成功 下面启动yarn 1sbin/start-yarn.sh 启动完成后 在浏览器上访问可视化页面：http://192.168.128.172:50070 到此为止，hadoop配置就结束了，谢谢大家的阅读。]]></content>
      <categories>
        <category>搭建自己的hadoop学习集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习]]></title>
    <url>%2F2019%2F08%2F06%2FLinux%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前言对于之前写的hadoop伪分布式的安装文章，可能对一些没有Linux基础的读者来说有一些的难度，所以建议大家先了解Linux的一些基础知识在来看之前的文章，可能就会觉得很简单了 学习建议我个人是比较推荐大家可以去b站看看韩顺平老师的Linux教程，可以在b站直接找到，这里放出b站链接 https://www.bilibili.com/video/av21303002?from=search&amp;seid=3872508779266125537 视频是18年上传的，但是知识是不会过时的，当然如果大家不想那么系统的了解Linux的话，可以去看看这个老哥的博客文章，这里放出链接 https://blog.csdn.net/weixin_41710054/article/details/89081599#22_vivim_19 这个文章写的比较的细，基本不知道的命令或者是快捷键都可以去文章中看看，自己看文章学习，可以比看视频省下不少时间，里面的命令可以基本满足正常操作Linux系统的要求。 好的，本次Linux的学习建议就到这里了，希望大家生活愉快。]]></content>
      <categories>
        <category>自己学习Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop伪分布式安装]]></title>
    <url>%2F2019%2F07%2F25%2Fhadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[前言本文介绍的主要是Hadoop的伪分布式的搭建以及遇到的相关问题的解决，做一下记录，jdk的安装这里就不做太多的介绍了，相信大家根据网上的安装介绍很快就能安装成功。 hadoop集群分为3种模型 单机模型：测试使用 伪分布式模型：运行于单机 完全分布式模型：适用于多台机器 以下是使用的环境 操作环境 主机名 IP地址 jdk hadoop版本 centos6.8 hadoop101 192.168.128.171 jdk1.8.0_191 hadoop-2.7.2 本文中使用的各种包，后续我会进行上传，以方便读者的使用 安装hadoophadoop上传与解压​ 当我们配置好自己的虚拟机后，可以自行在网上下载xftp和xshell，来对于自己的虚拟机进行远程上传文件和远程操作，这两款软件对于学生而言都是免费的，大家可以自行在网站上下载，速度可能会有点慢。 ​ 当我们下载好这两款软件后，就可以将hadoop的解压包上传至自己的虚拟机上去，我们将解压包上传至/opt/software中，开始解压hadoop，将hadoop解压至/opt/module/中，同时建议将java也解压至/opt/module/中，方便后面的管理。 配置环境变量1vi /etc/profile 具体配置 在/etc/profile的最后面加上，关于vi编辑器的用法可以自行百度一下，简单用法应该几分钟就能学会 123456export JAVA_HOME=/opt/module/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/opt/module/hadoop-2.7.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin 在配置环境变量完成后，记得要进行让它生效 输入 1source /etc/profile 即可生效 可以输入 1hadoop version 如果成功则显示 123456Hadoop 2.7.2Subversion Unknown -r UnknownCompiled by root on 2017-05-22T10:49ZCompiled with protoc 2.5.0From source with checksum d0fda26633fa762bff87ec759ebe689cThis command was run using /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar 到这里hadoop就算是安装好了 配置hadoop在伪分布式里我们只需要改三个配置文件core-site.xml和hdfs-site.xml还有hadoop-env.sh 这三个文件在hadoop目录下的etc/hadoop文件夹下 core-site.xml文件包含了NameNode主机地址，监听端口等信息，对于这个伪分布式模型来说，我的主机地址为hadoo101，NameNode默认使用的端口为8020。 修改core-site.xml 123456789101112&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop101:8020&lt;/value&gt;&lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml用于配置/HDFS的相关属性，例如数据块的副本参数，数据块的副本对于伪分布式来说应该为1 修改hdfs-site.xml 1234567&lt;configuration&gt;&lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop-env.sh 由于Hadoop是java进程，所以需要添加jdk 修改hadoop-env.sh 1export JAVA_HOME=/opt/module/jdk1.8.0_191 对于伪分布式来说，改这三个配置文件够了。 在配置文件完成后，我们需要对hadoop进行初始化 在hadoop-2.7.2的目录下输入 1bin/hdfs namenode -format 如果初始化成功的话，一个和下图相似 到这里hadoop的配置就已经完成了 启动集群在hadoop-2.7.2目录下输入 1sbin/start-dfs.sh 启动dfs 输入 1sbin/start-yarn.sh 启动yarn节点 启动成功应该和下图相似 到这里我们的集群就算是启动成功了 我们可以在web端查看HDFS文件系统 http://192.168.128.171:50070 192.168.128.171是我的ip地址，如果配置的不同，改一下即可 web端的hdfs文件系统如下图所示 测试集群在HDFS文件系统上创建一个input文件夹 输入 1hadoop fs -mkdir /input 在web端应该可以看到 我们上传一个文件看看 1hadoop fs -put LICENSE.txt /input LICENSE.txt是hadoop自带的一个TXT文件 如果上传成功在web端应该可以看到 这样就是上传成功了 我们在HDFS上跑一下MapReduce程序 输入 1hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /input/LICENSE.txt /output 这里说明一下MapReduce要在启动yarn下运行 查看运行结果 在web端： part-r-00000这个就是运行出来的结果 我们可以使用命令行查看结果也可以把这个文件下载到本地，这里我们使用命令行查看 输入 1bin/hdfs dfs -cat /output/part-r-00000 返回如下结果 到这里基本可以了，我们的hadoop已经安装配置好了，可以进行下一步的学习了 关于MapReduce的WordCount程序详解可以看这个 https://blog.csdn.net/gulu_gulu_jp/article/details/51298164/ 本次伪分布的配置就到这里了，如果还有问题可以向我留言，谢谢阅读，下次的文章应该是完全分布式的hadoop的安装教程了]]></content>
      <categories>
        <category>搭建自己的hadoop学习集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
