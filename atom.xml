<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>欢迎光临</title>
  
  <subtitle>https://nghqqa.cn/</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="www.nghqqa.cn/"/>
  <updated>2019-08-11T09:33:58.326Z</updated>
  <id>www.nghqqa.cn/</id>
  
  <author>
    <name>逆光海</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>hadoop完全分布式安装</title>
    <link href="www.nghqqa.cn/2019/08/10/hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/"/>
    <id>www.nghqqa.cn/2019/08/10/hadoop完全分布式安装/</id>
    <published>2019-08-10T13:43:42.000Z</published>
    <updated>2019-08-11T09:33:58.326Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我们之前搭建了hadoop的伪分布式的集群，伪分布式的集群对于学习一些基础的hadoop操作是足够的，但是当你涉及到更复杂的操作时，伪分布式就不够了，完全分布式的性能比伪分布的要强，而且完全分布式可操作性也更高，与此同时完全分布式对于电脑的配置也要求更高，完全分布式一般需要3台虚拟机来完成,下面我们开始搭建自己的完全分布式。</p><h2 id="使用环境"><a href="#使用环境" class="headerlink" title="使用环境"></a>使用环境</h2><p>下面是本次搭建使用的环境</p><table><thead><tr><th align="left">操作环境</th><th>主机名</th><th>IP地址</th><th>jdk</th><th align="center">hadoop版本</th><th></th></tr></thead><tbody><tr><td align="left">centos6.8</td><td>hadoop102</td><td>192.168.128.172</td><td>jdk1.8.0_191</td><td align="center">hadoop-2.7.2</td><td></td></tr><tr><td align="left"></td><td>hadoop103</td><td>192.168.128.173</td><td></td><td align="center"></td><td></td></tr><tr><td align="left"></td><td></td><td>192.168.128.174</td><td></td><td align="center"></td><td></td></tr></tbody></table><p>​                    </p><p>本文中使用的各种包，后续我会进行上传，以方便读者的使用</p><h2 id="搭建步骤详解"><a href="#搭建步骤详解" class="headerlink" title="搭建步骤详解"></a>搭建步骤详解</h2><h3 id="1-修改各节点的网络配置"><a href="#1-修改各节点的网络配置" class="headerlink" title="1.修改各节点的网络配置"></a>1.修改各节点的网络配置</h3><p>在虚拟机中输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure><p>可以进入虚拟机的网卡配置</p><p>我们需要修改虚拟机的网卡默认配置，将我们的虚拟机的网卡配置设置为静态ip</p><p>修改配置可以如图所示</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/6mobrzRqbbHQ.png?imageslim" alt="mark"></p><p>对其他两个hadoop节点也同样做上述操作，只不过在IPADDR值不一样，分别填其节点对应的ip</p><h3 id="2-修改节点主机名，并且添加各节点映射"><a href="#2-修改节点主机名，并且添加各节点映射" class="headerlink" title="2.修改节点主机名，并且添加各节点映射"></a>2.修改节点主机名，并且添加各节点映射</h3><p>在命令行中输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc//sysconfig/network</span><br></pre></td></tr></table></figure><p>进入文件中修改hostname名称，如图所示</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/vzbBoY0iVkob.png?imageslim" alt="mark"></p><p>在其他两个子节点的hostname处分别填hadoop103和hadoop104</p><p>添加节点映射，输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p>添加节点映射为</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/cbqc8daw1Izy.png?imageslim" alt="mark"></p><p>我这里是多写了一些，对于这次搭建我们只需要添加hadoop102，hadoop103，hadoop104的节点映射即可</p><h2 id="3-关闭防火墙"><a href="#3-关闭防火墙" class="headerlink" title="3.关闭防火墙"></a>3.关闭防火墙</h2><p>我们只有关闭防火墙后才能在三台机器之间互相通信</p><p>所以关闭防火墙是很有必要的</p><p>我们可以使用这条命令来检查我们虚拟机开机时的防火墙状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables --list</span><br></pre></td></tr></table></figure><p>如果是已经关闭应该会如下图所示</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/NNdzq6Nt2Prm.png?imageslim" alt="mark"></p><p>如果没有关闭我们可以使用这两条命令来关闭我们的防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>这条命令是在本次虚拟机开启过程中关闭防火墙，也就是一次性关闭</p><p>我们还需要这条命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables  off</span><br></pre></td></tr></table></figure><p>禁止防火墙关机自启动，这样防火墙就是是关闭了</p><p>当hadoop102关闭防火墙后，对于hadoop103与hadoop104也要做同要的操作</p><p>在防火墙关闭完成后，输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure><p>重启虚拟机，检查防火墙是否已经关闭</p><h2 id="4-配置节点间ssh免密登陆"><a href="#4-配置节点间ssh免密登陆" class="headerlink" title="4.配置节点间ssh免密登陆"></a>4.配置节点间ssh免密登陆</h2><p>在hadoop102上输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>一直按回车</p><p>完成后在保证三台虚拟机开启且完成之前所有配置的情况下输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# ssh-copy-id hadoop102</span><br><span class="line">[root@hadoop102 ~]# ssh-copy-id hadoop103</span><br><span class="line">[root@hadoop102 ~]# ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure><p>在hadoop102上完成后，在其他两个节点上重复上述操作</p><p>验证ssh免密登录是否成功</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/djEjODwXsuF8.png?imageslim" alt="mark"></p><p>这里可以看到我们可以自己使用ssh转到hadoop103这台机器上</p><h2 id="5-安装java和hadoop"><a href="#5-安装java和hadoop" class="headerlink" title="5.安装java和hadoop"></a>5.安装java和hadoop</h2><p>我们先使用xftp将hadoop和java的压缩包上传到我们新建的/opt/software上同时新建一个module文件夹放置解压后的hadoop和java，新建文件夹代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/</span><br><span class="line">[root@hadoop102 ~]# mkdir software</span><br><span class="line">[root@hadoop102 ~]# mkdir module</span><br></pre></td></tr></table></figure><p>上传完成之后我们需要解压java和hadoop到/opt/module下，以便未来的管理</p><p>解压代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf jdk-8u191-linux-x64.tar.gz -C /opt/module/</span><br><span class="line">tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>解压完成后在/opt/module下应该是这样的，如图所示</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/tVfrEHmhKgeh.png?imageslim" alt="mark"></p><p>之后我们就需要配置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br></pre></td></tr></table></figure><p>在最后添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><p>退出后，输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>使其生效</p><p>验证java和hadoop环境变量是否配置完成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 module]# java -version</span><br><span class="line">java version &quot;1.8.0_191&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_191-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br><span class="line">[root@hadoop103 module]# hadoop version</span><br><span class="line">Hadoop 2.7.2</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2017-05-22T10:49Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum d0fda26633fa762bff87ec759ebe689c</span><br><span class="line">This command was run using /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar</span><br><span class="line">[root@hadoop103 module]#</span><br></pre></td></tr></table></figure><p>环境变量已经配置完成， 在其他两个节点上重复上述操作</p><h2 id="6-配置hadoop中的文件"><a href="#6-配置hadoop中的文件" class="headerlink" title="6.配置hadoop中的文件"></a>6.配置hadoop中的文件</h2><h3 id="6-1配置文件core-site-xml"><a href="#6-1配置文件core-site-xml" class="headerlink" title="6.1配置文件core-site.xml"></a>6.1配置文件core-site.xml</h3><p><strong>core-site.xml文件包含了NameNode主机地址，监听端口等信息，对于这个伪分布式模型来说，我的主机地址为hadoo101，NameNode默认使用的端口为8020。</strong></p><p>修改core-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hadoop101:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">        &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="6-2配置文件hdfs-site-xml"><a href="#6-2配置文件hdfs-site-xml" class="headerlink" title="6.2配置文件hdfs-site.xml"></a>6.2配置文件hdfs-site.xml</h3><p><strong>hdfs-site.xml用于配置/HDFS的相关属性，例如数据块的副本参数，数据块的副本对于完全分布式来说应该为3</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop104:50090&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="6-3配置文件slaves"><a href="#6-3配置文件slaves" class="headerlink" title="6.3配置文件slaves"></a>6.3配置文件slaves</h3><p><strong>slaves文件里面记录的是集群里所有DataNode的主机名</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]#vim slaves</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><h3 id="6-4配置文件yarn-site-xml"><a href="#6-4配置文件yarn-site-xml" class="headerlink" title="6.4配置文件yarn-site.xml"></a>6.4配置文件yarn-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">&lt;!-- reducer获取数据的方式 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop103&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="6-5配置文件yarn-env-sh"><a href="#6-5配置文件yarn-env-sh" class="headerlink" title="6.5配置文件yarn-env.sh"></a>6.5配置文件yarn-env.sh</h3><p>在其中修改java的路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br></pre></td></tr></table></figure><h3 id="6-6配置hadoop-env-sh"><a href="#6-6配置hadoop-env-sh" class="headerlink" title="6.6配置hadoop-env.sh"></a>6.6配置hadoop-env.sh</h3><p><strong>hadoop-env.sh 由于Hadoop是java进程，所以需要添加jdk</strong></p><p>修改hadoop-env.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br></pre></td></tr></table></figure><h3 id="6-7配置文件mapred-site-xml"><a href="#6-7配置文件mapred-site-xml" class="headerlink" title="6.7配置文件mapred-site.xml"></a>6.7配置文件mapred-site.<strong>xml</strong></h3><p>先改名，因为本身是没有mapred-site.xml这个文件的</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p>改名完成后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]#vim mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定mr运行在yarn上 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>这些配置文件改好后，返回/opt/module目录下</p><p>把hadoop102下修改的文件分发到hadoop103和hadoop104下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]# scp -r hadoop root@hadoop103:/opt/module/</span><br><span class="line">[root@master local]# scp -r hadoop root@hadoop104:/opt/module/</span><br></pre></td></tr></table></figure><h2 id="7-测试集群"><a href="#7-测试集群" class="headerlink" title="7.测试集群"></a>7.测试集群</h2><p>在完成配置文件等一系列工作后，我们要开始测试集群了</p><p>先格式化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop2 hadoop-2.7.2]# bin/hdfs namenode –format</span><br></pre></td></tr></table></figure><p>之后启动hdfs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop2 hadoop-2.7.2]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[root@hadoop2 hadoop-2.7.2]# jps</span><br><span class="line">4166 NameNode</span><br><span class="line">4482 Jps</span><br><span class="line">4263 DataNode</span><br><span class="line"></span><br><span class="line">[root@hadoop3 桌面]# jps</span><br><span class="line">3218 DataNode</span><br><span class="line">3288 Jps</span><br><span class="line"></span><br><span class="line">[root@hadoop4 桌面]# jps</span><br><span class="line">3221 DataNode</span><br><span class="line">3283 SecondaryNameNode</span><br><span class="line">3364 Jps</span><br></pre></td></tr></table></figure><p>如果是这样这表示启动hdfs成功</p><p>下面启动yarn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>启动完成后</p><p>在浏览器上访问可视化页面：<a href="http://192.168.128.172:50070" target="_blank" rel="noopener">http://192.168.128.172:50070</a></p><p><img src="http://qqa.nghqqa.cn/blog/20190811/CFi7BSr2G8Ws.png?imageslim" alt="mark"></p><p>到此为止，hadoop配置就结束了，谢谢大家的阅读。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;我们之前搭建了hadoop的伪分布式的集群，伪分布式的集群对于学习一些基础的hadoop操作是足够的，但是当你涉及到更复杂的操作时，伪分布式
      
    
    </summary>
    
      <category term="搭建自己的hadoop学习集群" scheme="www.nghqqa.cn/categories/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hadoop%E5%AD%A6%E4%B9%A0%E9%9B%86%E7%BE%A4/"/>
    
    
      <category term="hadoop" scheme="www.nghqqa.cn/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Linux学习</title>
    <link href="www.nghqqa.cn/2019/08/06/Linux%E5%AD%A6%E4%B9%A0/"/>
    <id>www.nghqqa.cn/2019/08/06/Linux学习/</id>
    <published>2019-08-06T11:28:07.000Z</published>
    <updated>2019-08-06T11:37:21.155Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>对于之前写的hadoop伪分布式的安装文章，可能对一些没有Linux基础的读者来说有一些的难度，所以建议大家先了解Linux的一些基础知识在来看之前的文章，可能就会觉得很简单了</p><h2 id="学习建议"><a href="#学习建议" class="headerlink" title="学习建议"></a>学习建议</h2><p>我个人是比较推荐大家可以去b站看看韩顺平老师的Linux教程，可以在b站直接找到，这里放出b站链接</p><p><a href="https://www.bilibili.com/video/av21303002?from=search&amp;seid=3872508779266125537" target="_blank" rel="noopener">https://www.bilibili.com/video/av21303002?from=search&amp;seid=3872508779266125537</a></p><p>视频是18年上传的，但是知识是不会过时的，当然如果大家不想那么系统的了解Linux的话，可以去看看这个老哥的博客文章，这里放出链接</p><p><a href="https://blog.csdn.net/weixin_41710054/article/details/89081599#22_vivim_19" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41710054/article/details/89081599#22_vivim_19</a></p><p>这个文章写的比较的细，基本不知道的命令或者是快捷键都可以去文章中看看，自己看文章学习，可以比看视频省下不少时间，里面的命令可以基本满足正常操作Linux系统的要求。</p><p>好的，本次Linux的学习建议就到这里了，希望大家生活愉快。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;对于之前写的hadoop伪分布式的安装文章，可能对一些没有Linux基础的读者来说有一些的难度，所以建议大家先了解Linux的一些基础知识在
      
    
    </summary>
    
      <category term="自己学习Linux" scheme="www.nghqqa.cn/categories/%E8%87%AA%E5%B7%B1%E5%AD%A6%E4%B9%A0Linux/"/>
    
    
      <category term="Linux" scheme="www.nghqqa.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>hadoop伪分布式安装</title>
    <link href="www.nghqqa.cn/2019/07/25/hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/"/>
    <id>www.nghqqa.cn/2019/07/25/hadoop伪分布式安装/</id>
    <published>2019-07-25T05:38:40.000Z</published>
    <updated>2019-08-11T08:47:11.572Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文介绍的主要是Hadoop的伪分布式的搭建以及遇到的相关问题的解决，做一下记录，jdk的安装这里就不做太多的介绍了，相信大家根据网上的安装介绍很快就能安装成功。</p><p>hadoop集群分为3种模型</p><ol><li>单机模型：测试使用 </li><li>伪分布式模型：运行于单机 </li><li>完全分布式模型：适用于多台机器</li></ol><p>以下是使用的环境</p><table><thead><tr><th align="left">操作环境</th><th>主机名</th><th>IP地址</th><th>jdk</th><th align="center">hadoop版本</th><th></th></tr></thead><tbody><tr><td align="left">centos6.8</td><td>hadoop101</td><td>192.168.128.171</td><td>jdk1.8.0_191</td><td align="center">hadoop-2.7.2</td><td></td></tr><tr><td align="left"></td><td></td><td></td><td></td><td align="center"></td><td></td></tr></tbody></table><p>本文中使用的各种包，后续我会进行上传，以方便读者的使用</p><h1 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a>安装hadoop</h1><h2 id="hadoop上传与解压"><a href="#hadoop上传与解压" class="headerlink" title="hadoop上传与解压"></a>hadoop上传与解压</h2><p>​    当我们配置好自己的虚拟机后，可以自行在网上下载xftp和xshell，来对于自己的虚拟机进行远程上传文件和远程操作，这两款软件对于学生而言都是免费的，大家可以自行在网站上下载，速度可能会有点慢。</p><p>​    当我们下载好这两款软件后，就可以将hadoop的解压包上传至自己的虚拟机上去，我们将解压包上传至/opt/software中，开始解压hadoop，将hadoop解压至/opt/module/中，同时建议将java也解压至/opt/module/中，方便后面的管理。</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br></pre></td></tr></table></figure><p>具体配置</p><p>在/etc/profile的最后面加上，关于vi编辑器的用法可以自行百度一下，简单用法应该几分钟就能学会</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><p>在配置环境变量完成后，记得要进行让它生效</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>即可生效</p><p>可以输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure><p>如果成功则显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 2.7.2</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2017-05-22T10:49Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum d0fda26633fa762bff87ec759ebe689c</span><br><span class="line">This command was run using /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar</span><br></pre></td></tr></table></figure><p>到这里hadoop就算是安装好了</p><h1 id="配置hadoop"><a href="#配置hadoop" class="headerlink" title="配置hadoop"></a>配置hadoop</h1><p>在伪分布式里我们只需要改三个配置文件core-site.xml和hdfs-site.xml还有hadoop-env.sh</p><p>这三个文件在hadoop目录下的etc/hadoop文件夹下</p><p><strong>core-site.xml文件包含了NameNode主机地址，监听端口等信息，对于这个伪分布式模型来说，我的主机地址为hadoo101，NameNode默认使用的端口为8020。</strong></p><p>修改core-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hadoop101:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">        &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>hdfs-site.xml用于配置/HDFS的相关属性，例如数据块的副本参数，数据块的副本对于伪分布式来说应该为1</strong></p><p>修改hdfs-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定HDFS副本的数量 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>hadoop-env.sh 由于Hadoop是java进程，所以需要添加jdk</strong></p><p>修改hadoop-env.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br></pre></td></tr></table></figure><p>对于伪分布式来说，改这三个配置文件够了。</p><p>在配置文件完成后，我们需要对hadoop进行初始化</p><p>在hadoop-2.7.2的目录下输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果初始化成功的话，一个和下图相似</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/9c98NJ18De35.png?imageslim" alt="mark"></p><p>到这里hadoop的配置就已经完成了</p><h1 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h1><p>在hadoop-2.7.2目录下输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>启动dfs</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>启动yarn节点</p><p>启动成功应该和下图相似</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/15Fa0e8qUDCR.png?imageslim" alt="mark"></p><p>到这里我们的集群就算是启动成功了</p><p>我们可以在web端查看HDFS文件系统</p><p><a href="http://192.168.128.171:50070/" target="_blank" rel="noopener">http://192.168.128.171:50070</a></p><p>192.168.128.171是我的ip地址，如果配置的不同，改一下即可</p><p>web端的hdfs文件系统如下图所示</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/fKgRPKMecBlX.png?imageslim" alt="mark"></p><h1 id="测试集群"><a href="#测试集群" class="headerlink" title="测试集群"></a>测试集群</h1><p>在HDFS文件系统上创建一个input文件夹</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input</span><br></pre></td></tr></table></figure><p>在web端应该可以看到</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/GK25hp1DN8GK.png?imageslim" alt="mark"></p><p>我们上传一个文件看看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put LICENSE.txt /input</span><br></pre></td></tr></table></figure><p>LICENSE.txt是hadoop自带的一个TXT文件</p><p>如果上传成功在web端应该可以看到</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/C4OldlfJiDzs.png?imageslim" alt="mark"></p><p>这样就是上传成功了</p><p>我们在HDFS上跑一下MapReduce程序</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /input/LICENSE.txt /output</span><br></pre></td></tr></table></figure><p>这里说明一下MapReduce要在启动yarn下运行</p><p>查看运行结果</p><p>在web端：</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/4wgLyU3syVSv.png?imageslim" alt="mark"></p><p>part-r-00000这个就是运行出来的结果</p><p>我们可以使用命令行查看结果也可以把这个文件下载到本地，这里我们使用命令行查看</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -cat /output/part-r-00000</span><br></pre></td></tr></table></figure><p>返回如下结果</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/FHhHjKyLhTk9.png?imageslim" alt="mark"></p><p>到这里基本可以了，我们的hadoop已经安装配置好了，可以进行下一步的学习了</p><p>关于MapReduce的WordCount程序详解可以看这个</p><p><a href="https://blog.csdn.net/gulu_gulu_jp/article/details/51298164/" target="_blank" rel="noopener">https://blog.csdn.net/gulu_gulu_jp/article/details/51298164/</a></p><p>本次伪分布的配置就到这里了，如果还有问题可以向我留言，谢谢阅读，下次的文章应该是完全分布式的hadoop的安装教程了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文介绍的主要是Hadoop的伪分布式的搭建以及遇到的相关问题的解决，做一下记录，jdk的安装这里就不做太多的介绍了，相信大家根据网上的安装
      
    
    </summary>
    
      <category term="搭建自己的hadoop学习集群" scheme="www.nghqqa.cn/categories/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hadoop%E5%AD%A6%E4%B9%A0%E9%9B%86%E7%BE%A4/"/>
    
    
      <category term="hadoop" scheme="www.nghqqa.cn/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="www.nghqqa.cn/2019/07/22/hello-world/"/>
    <id>www.nghqqa.cn/2019/07/22/hello-world/</id>
    <published>2019-07-21T23:42:50.603Z</published>
    <updated>2019-07-25T13:27:10.775Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
