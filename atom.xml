<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>欢迎光临</title>
  
  <subtitle>https://nghqqa.cn/</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="www.nghqqa.cn/"/>
  <updated>2019-08-14T12:16:32.357Z</updated>
  <id>www.nghqqa.cn/</id>
  
  <author>
    <name>逆光海</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用的hadoop相关安装包</title>
    <link href="www.nghqqa.cn/2019/08/14/%E4%BD%BF%E7%94%A8%E7%9A%84hadoop%E7%9B%B8%E5%85%B3%E5%AE%89%E8%A3%85%E5%8C%85/"/>
    <id>www.nghqqa.cn/2019/08/14/使用的hadoop相关安装包/</id>
    <published>2019-08-14T05:29:40.000Z</published>
    <updated>2019-08-14T12:16:32.357Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这里放一些之前安装用到的安装包</p><p>链接：<a href="https://pan.baidu.com/s/1_qp_AKhRJNvzr6P63nJ9Rw" target="_blank" rel="noopener">https://pan.baidu.com/s/1_qp_AKhRJNvzr6P63nJ9Rw</a><br>提取码：7s9n </p><table><thead><tr><th>hadoop</th><th>spark</th><th>hive</th><th>scala</th></tr></thead><tbody><tr><td>2.7.2</td><td>spark-2.4.0</td><td>hive-1.1.0</td><td>scala-2.11.4</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;这里放一些之前安装用到的安装包&lt;/p&gt;
&lt;p&gt;链接：&lt;a href=&quot;https://pan.baidu.com/s/1_qp_AKhRJN
      
    
    </summary>
    
      <category term="相关安装包" scheme="www.nghqqa.cn/categories/%E7%9B%B8%E5%85%B3%E5%AE%89%E8%A3%85%E5%8C%85/"/>
    
    
      <category term="hadoop" scheme="www.nghqqa.cn/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>scala的安装</title>
    <link href="www.nghqqa.cn/2019/08/13/scala%E7%9A%84%E5%AE%89%E8%A3%85/"/>
    <id>www.nghqqa.cn/2019/08/13/scala的安装/</id>
    <published>2019-08-13T05:51:20.000Z</published>
    <updated>2019-08-13T13:07:23.817Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前写的spark的安装中忘记说明安装spark要先安装scala，因为spark的底层是使用scala脚本语言开发</p><p>使用版本</p><table><thead><tr><th>scala-2.11.4</th></tr></thead><tbody><tr><td></td></tr></tbody></table><h2 id="安装spark"><a href="#安装spark" class="headerlink" title="安装spark"></a>安装spark</h2><h3 id="上传解压spark"><a href="#上传解压spark" class="headerlink" title="上传解压spark"></a>上传解压spark</h3><p>spark的压缩包上传到我们的/opt/software上，解压到/opt/module/</p><p>代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf scala-2.11.4.tgz -C /opt/module/</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>在末尾添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#scala</span><br><span class="line">export SCALA_HOME=/opt/module/scala-2.11.4</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span><br></pre></td></tr></table></figure><p>记得保存使其生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala -version</span><br></pre></td></tr></table></figure><p>出现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Scala code runner version 2.11.4 -- Copyright 2002-2013, LAMP/EPFL</span><br></pre></td></tr></table></figure><p>则代表安装完成</p><p>之后在其他两个节点中重复上述步骤！</p><p>安装scala到这里也就完成，一定要安装完scala后去安装spark，谢谢大家的阅读。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;之前写的spark的安装中忘记说明安装spark要先安装scala，因为spark的底层是使用scala脚本语言开发&lt;/p&gt;
&lt;p&gt;使用版本
      
    
    </summary>
    
      <category term="搭建自己的hadoop学习集群" scheme="www.nghqqa.cn/categories/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hadoop%E5%AD%A6%E4%B9%A0%E9%9B%86%E7%BE%A4/"/>
    
    
      <category term="hadoop" scheme="www.nghqqa.cn/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>spark安装</title>
    <link href="www.nghqqa.cn/2019/08/13/spark%E7%9A%84%E5%AE%89%E8%A3%85/"/>
    <id>www.nghqqa.cn/2019/08/13/spark的安装/</id>
    <published>2019-08-13T03:40:20.000Z</published>
    <updated>2019-08-13T05:50:27.149Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言——spark介绍"><a href="#前言——spark介绍" class="headerlink" title="前言——spark介绍"></a>前言——spark介绍</h1><p>Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎，是类似于Hadoop MapReduce的通用并行框架。Spark拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是——Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。Spark实际上是对Hadoop的一种补充，可以很好的在Hadoop 文件系统中并行运行。</p><h2 id="安装spark"><a href="#安装spark" class="headerlink" title="安装spark"></a>安装spark</h2><table><thead><tr><th>安装版本</th></tr></thead><tbody><tr><td>spark-2.4.0</td></tr></tbody></table><h3 id="上传解压spark"><a href="#上传解压spark" class="headerlink" title="上传解压spark"></a>上传解压spark</h3><p>我们将spark上传到/opt/software下，之后将其解压到/opt/module/</p><p>解压代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf spark-2.4.0-bin-hadoop2.7.tgz -C /opt/module/</span><br></pre></td></tr></table></figure><p>修改名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-2.4.0-bin-hadoop2.7 spark-2.4.0</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>在/etc/profile文件的最后添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/opt/module/spark-2.4.0</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>记得保存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="修改spark-env-sh文件"><a href="#修改spark-env-sh文件" class="headerlink" title="修改spark-env.sh文件"></a>修改spark-env.sh<strong>文件</strong></h2><p>进入spark文件夹下的conf文件夹，修改文件名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>改完之后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure><p>在spark-env.sh文件的末尾添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br><span class="line">export SCALA_HOME=/opt/module/scala-2.11.4</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export HADOOP_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure><h2 id="修改-slaves"><a href="#修改-slaves" class="headerlink" title="修改 slaves"></a>修改 slaves</h2><p> 修改 slaves 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv slaves.template slaves</span><br></pre></td></tr></table></figure><p> 打开 slaves 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p>添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p>完成后，我们将spark文件夹传给slave1和slave2</p><p>输入（传输的时候要在spark-2.4.0的上一目录下传输）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r spark-2.4.0 root@salve1:/opt/module/</span><br><span class="line">scp -r spark-2.4.0 root@salve2:/opt/module/</span><br></pre></td></tr></table></figure><p>传输完成，在slave1和slave2中配置环境变量</p><h2 id="启动spark"><a href="#启动spark" class="headerlink" title="启动spark"></a>启动spark</h2><p>在spark-2.4.0目录下输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure><p>启动完成后，如果可以访问</p><p><a href="http://192.168.128.172:8080/" target="_blank" rel="noopener">http://192.168.128.172:8080/</a></p><p>如图</p><p><img src="http://qqa.nghqqa.cn/blog/20190813/XVXaymkIosSi.png?imageslim" alt="mark"></p><p>则为成功</p><p>到这里spark的安装也就完成了，谢谢大家的阅读。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言——spark介绍&quot;&gt;&lt;a href=&quot;#前言——spark介绍&quot; class=&quot;headerlink&quot; title=&quot;前言——spark介绍&quot;&gt;&lt;/a&gt;前言——spark介绍&lt;/h1&gt;&lt;p&gt;Apache Spark 是专为大规模数据处理而设计的快速通用的计算
      
    
    </summary>
    
      <category term="搭建自己的hadoop学习集群" scheme="www.nghqqa.cn/categories/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hadoop%E5%AD%A6%E4%B9%A0%E9%9B%86%E7%BE%A4/"/>
    
    
      <category term="hadoop" scheme="www.nghqqa.cn/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>hive的安装</title>
    <link href="www.nghqqa.cn/2019/08/13/hive%E7%9A%84%E5%AE%89%E8%A3%85/"/>
    <id>www.nghqqa.cn/2019/08/13/hive的安装/</id>
    <published>2019-08-13T00:17:40.000Z</published>
    <updated>2019-08-13T01:07:09.947Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言——hive介绍"><a href="#前言——hive介绍" class="headerlink" title="前言——hive介绍"></a>前言——hive介绍</h1><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过和SQL类似的HiveQL语言快速实现简单的MapReduce统计,不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。</p><p>Hive 没有专门的数据格式。所有Hive 的数据都存储在Hadoop兼容的文件系统（例如HDFS）中。Hive 在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive 设定的目录下，因此，Hive 不支持对数据的改写和添加，所有的数据都是在加载的时候确定的。</p><p>附一张hadoop的生态圈图</p><p><img src="http://qqa.nghqqa.cn/blog/20190813/7gwkKD6NYb3T.png?imageslim" alt="mark"></p><h2 id="安装hive"><a href="#安装hive" class="headerlink" title="安装hive"></a>安装hive</h2><p>hive的安装基于之前的搭建的hadoop完全分布式集群，只需要安装在hadoop102上就行</p><table><thead><tr><th>安装hive版本</th></tr></thead><tbody><tr><td>hive-1.1.0</td></tr></tbody></table><h3 id="上传解压hive"><a href="#上传解压hive" class="headerlink" title="上传解压hive"></a>上传解压hive</h3><p>我们将hive上传到/opt/software下，之后将其解压到/opt/module/</p><p>解压代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf apache-hive-1.1.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>解压完成后我们修改hive的文件名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv apache-hive-1.1.0-bin.tar.gz hive-1.1.0</span><br></pre></td></tr></table></figure><p>修改文件名是为了让我们在配置环境变量时更加的方便</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>在/etc/profile文件的最后添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/opt/module/hive-1.1.0</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/sbin</span><br></pre></td></tr></table></figure><p>配置完成后，记得保存使其生效</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h3><p>hive它有自己的内置数据库derby，但是hive 使用derby 数据库存在不支持多个连接的问题，所以我们一般会使用mysql来代替hive的元数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/module/src/</span><br><span class="line">[root@hadoop102 src]# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">[root@hadoop102 src]# rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">[root@hadoop102 src]# yum install mysql-community-server</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 这里时间较长，耐心等待...</span><br><span class="line"></span><br><span class="line"># 安装完成后，重启服务</span><br><span class="line">[root@hadoop102 src]# service mysqld restart</span><br><span class="line">[root@hadoop102 src]# mysql</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 3</span><br><span class="line">Server version: 5.6.42 MySQL Community Server (GPL)</span><br><span class="line">Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective owners.</span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line">mysql&gt;</span><br><span class="line"></span><br><span class="line"># mysql安装成功</span><br></pre></td></tr></table></figure><p>同时我们还需要去网上下载mysql的驱动包（mysql-connector-java.jar）把这个驱动包放置在hive目录下的lib目录下。</p><h3 id="修改hive-site-xml"><a href="#修改hive-site-xml" class="headerlink" title="修改hive-site.xml"></a>修改hive-site.xml</h3><p>hive的配置文件放置在/opt/module/hive-1.1.0/conf下</p><p>配置hive-site.xml(conf中可能没有这个文件，我们使用vim打开时，没有的话，vim会帮我们自动创建)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><p>在文件中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.metastore.local&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hivepwd&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="修改hive-env-sh"><a href="#修改hive-env-sh" class="headerlink" title="修改hive-env.sh"></a>修改hive-env.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 conf]# mv hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><p>在文件最后添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure><h3 id="配置mysql"><a href="#配置mysql" class="headerlink" title="配置mysql"></a>配置mysql</h3><p> 创建数据库 hive ，用来保存 Hive 元数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database hive;</span><br></pre></td></tr></table></figure><p>同时使 root 用户可以操作数据库 hive 中的所有表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GRANT all ON hive.* TO root@&apos;Hadoop102&apos; IDENTIFIED BY &apos;hivepwd&apos;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><p>这样<br>Hive 的元数据库就安装完成。</p><h2 id="测试hive安装是否成功"><a href="#测试hive安装是否成功" class="headerlink" title="测试hive安装是否成功"></a>测试hive安装是否成功</h2><p>启动Hadoop与mysql</p><p>输入hive</p><p>进入hive，出现命令行就说明之前搭建是成功的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Hadoop102 ]# hive</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>进入<br>Hive 命令行 执行命令 创建一个名为 test 的表 查询该表的记录数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table test(id int);</span><br><span class="line">select count(*) from test;</span><br></pre></td></tr></table></figure><p>如果查询结果为0，则成功</p><p><img src="http://qqa.nghqqa.cn/blog/20190813/8jcn3tWNMSgl.png?imageslim" alt="mark"></p><p>到这里，hive的安装也就完成了，谢谢大家的阅读。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言——hive介绍&quot;&gt;&lt;a href=&quot;#前言——hive介绍&quot; class=&quot;headerlink&quot; title=&quot;前言——hive介绍&quot;&gt;&lt;/a&gt;前言——hive介绍&lt;/h1&gt;&lt;p&gt;Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一
      
    
    </summary>
    
      <category term="搭建自己的hadoop学习集群" scheme="www.nghqqa.cn/categories/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hadoop%E5%AD%A6%E4%B9%A0%E9%9B%86%E7%BE%A4/"/>
    
    
      <category term="hadoop" scheme="www.nghqqa.cn/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>hadoop完全分布式安装</title>
    <link href="www.nghqqa.cn/2019/08/10/hadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/"/>
    <id>www.nghqqa.cn/2019/08/10/hadoop完全分布式安装/</id>
    <published>2019-08-10T13:43:42.000Z</published>
    <updated>2019-08-13T05:47:03.573Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我们之前搭建了hadoop的伪分布式的集群，伪分布式的集群对于学习一些基础的hadoop操作是足够的，但是当你涉及到更复杂的操作时，伪分布式就不够了，完全分布式的性能比伪分布的要强，而且完全分布式可操作性也更高，与此同时完全分布式对于电脑的配置也要求更高，完全分布式一般需要3台虚拟机来完成,下面我们开始搭建自己的完全分布式。</p><h2 id="使用环境"><a href="#使用环境" class="headerlink" title="使用环境"></a>使用环境</h2><p>下面是本次搭建使用的环境</p><table><thead><tr><th align="left">操作环境</th><th>主机名</th><th>IP地址</th><th>jdk</th><th align="center">hadoop版本</th><th></th></tr></thead><tbody><tr><td align="left">centos6.8</td><td>hadoop102</td><td>192.168.128.172</td><td>jdk1.8.0_191</td><td align="center">hadoop-2.7.2</td><td></td></tr><tr><td align="left"></td><td>hadoop103</td><td>192.168.128.173</td><td></td><td align="center"></td><td></td></tr><tr><td align="left"></td><td>hadoop104</td><td>192.168.128.174</td><td></td><td align="center"></td><td></td></tr></tbody></table><p>​                    </p><p>本文中使用的各种包，后续我会进行上传，以方便读者的使用</p><h2 id="搭建步骤详解"><a href="#搭建步骤详解" class="headerlink" title="搭建步骤详解"></a>搭建步骤详解</h2><h3 id="1-修改各节点的网络配置"><a href="#1-修改各节点的网络配置" class="headerlink" title="1.修改各节点的网络配置"></a>1.修改各节点的网络配置</h3><p>在虚拟机中输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure><p>可以进入虚拟机的网卡配置</p><p>我们需要修改虚拟机的网卡默认配置，将我们的虚拟机的网卡配置设置为静态ip</p><p>ip地址 根据 VMware 虚拟网络进行相关配置 如图</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/lihMS1cmJ5ds.png?imageslim" alt="mark"></p><p><img src="http://qqa.nghqqa.cn/blog/20190811/628Xq2EJFY3r.png?imageslim" alt="mark"></p><p>在虚拟机里修改配置可以如图所示</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/6mobrzRqbbHQ.png?imageslim" alt="mark"></p><p>修改完输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service network restart</span><br></pre></td></tr></table></figure><p>重启网络服务</p><p>对其他两个hadoop节点也同样做上述操作，只不过在IPADDR值不一样，分别填其节点对应的ip</p><h3 id="2-修改节点主机名，并且添加各节点映射"><a href="#2-修改节点主机名，并且添加各节点映射" class="headerlink" title="2.修改节点主机名，并且添加各节点映射"></a>2.修改节点主机名，并且添加各节点映射</h3><p>在命令行中输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc//sysconfig/network</span><br></pre></td></tr></table></figure><p>进入文件中修改hostname名称，如图所示</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/vzbBoY0iVkob.png?imageslim" alt="mark"></p><p>在其他两个子节点的hostname处分别填hadoop103和hadoop104</p><p>添加节点映射，输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><p>添加节点映射为</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/cbqc8daw1Izy.png?imageslim" alt="mark"></p><p>我这里是多写了一些，对于这次搭建我们只需要添加hadoop102，hadoop103，hadoop104的节点映射即可</p><h2 id="3-关闭防火墙"><a href="#3-关闭防火墙" class="headerlink" title="3.关闭防火墙"></a>3.关闭防火墙</h2><p>我们只有关闭防火墙后才能在三台机器之间互相通信</p><p>所以关闭防火墙是很有必要的</p><p>我们可以使用这条命令来检查我们虚拟机开机时的防火墙状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables --list</span><br></pre></td></tr></table></figure><p>如果是已经关闭应该会如下图所示</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/NNdzq6Nt2Prm.png?imageslim" alt="mark"></p><p>如果没有关闭我们可以使用这两条命令来关闭我们的防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br></pre></td></tr></table></figure><p>这条命令是在本次虚拟机开启过程中关闭防火墙，也就是一次性关闭</p><p>我们还需要这条命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables  off</span><br></pre></td></tr></table></figure><p>禁止防火墙关机自启动，这样防火墙就是是关闭了</p><p>当hadoop102关闭防火墙后，对于hadoop103与hadoop104也要做同样的操作</p><p>在防火墙关闭完成后，输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure><p>重启虚拟机，检查防火墙是否已经关闭</p><h2 id="4-配置节点间ssh免密登陆"><a href="#4-配置节点间ssh免密登陆" class="headerlink" title="4.配置节点间ssh免密登陆"></a>4.配置节点间ssh免密登陆</h2><p>在hadoop102上输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>一直按回车</p><p>完成后在保证三台虚拟机开启且完成之前所有配置的情况下输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# ssh-copy-id hadoop102</span><br><span class="line">[root@hadoop102 ~]# ssh-copy-id hadoop103</span><br><span class="line">[root@hadoop102 ~]# ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure><p>在hadoop102上完成后，在其他两个节点上重复上述操作</p><p>验证ssh免密登录是否成功</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/djEjODwXsuF8.png?imageslim" alt="mark"></p><p>这里可以看到我们可以自己使用ssh转到hadoop103这台机器上</p><h2 id="5-安装java和hadoop"><a href="#5-安装java和hadoop" class="headerlink" title="5.安装java和hadoop"></a>5.安装java和hadoop</h2><p>我们先使用xftp将hadoop和java的压缩包上传到我们新建的/opt/software上同时新建一个module文件夹放置解压后的hadoop和java，新建文件夹代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# cd /opt/</span><br><span class="line">[root@hadoop102 ~]# mkdir software</span><br><span class="line">[root@hadoop102 ~]# mkdir module</span><br></pre></td></tr></table></figure><p>上传完成之后我们需要解压java和hadoop到/opt/module下，以便未来的管理</p><p>解压代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf jdk-8u191-linux-x64.tar.gz -C /opt/module/</span><br><span class="line">tar -zxf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>解压完成后在/opt/module下应该是这样的，如图所示</p><p><img src="http://qqa.nghqqa.cn/blog/20190811/tVfrEHmhKgeh.png?imageslim" alt="mark"></p><p>之后我们就需要配置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br></pre></td></tr></table></figure><p>在最后添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><p>退出后，输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>使其生效</p><p>验证java和hadoop环境变量是否配置完成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop103 module]# java -version</span><br><span class="line">java version &quot;1.8.0_191&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_191-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br><span class="line">[root@hadoop103 module]# hadoop version</span><br><span class="line">Hadoop 2.7.2</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2017-05-22T10:49Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum d0fda26633fa762bff87ec759ebe689c</span><br><span class="line">This command was run using /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar</span><br><span class="line">[root@hadoop103 module]#</span><br></pre></td></tr></table></figure><p>环境变量已经配置完成， 在其他两个节点上重复上述操作</p><h2 id="6-配置hadoop中的文件"><a href="#6-配置hadoop中的文件" class="headerlink" title="6.配置hadoop中的文件"></a>6.配置hadoop中的文件</h2><h3 id="6-1配置文件core-site-xml"><a href="#6-1配置文件core-site-xml" class="headerlink" title="6.1配置文件core-site.xml"></a>6.1配置文件core-site.xml</h3><p><strong>core-site.xml文件包含了NameNode主机地址，监听端口等信息，对于这个伪分布式模型来说，我的主机地址为hadoo101，NameNode默认使用的端口为8020。</strong></p><p>修改core-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hadoop101:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">        &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="6-2配置文件hdfs-site-xml"><a href="#6-2配置文件hdfs-site-xml" class="headerlink" title="6.2配置文件hdfs-site.xml"></a>6.2配置文件hdfs-site.xml</h3><p><strong>hdfs-site.xml用于配置/HDFS的相关属性，例如数据块的副本参数，数据块的副本对于完全分布式来说应该为3</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">&lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop104:50090&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="6-3配置文件slaves"><a href="#6-3配置文件slaves" class="headerlink" title="6.3配置文件slaves"></a>6.3配置文件slaves</h3><p><strong>slaves文件里面记录的是集群里所有DataNode的主机名</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]#vim slaves</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><h3 id="6-4配置文件yarn-site-xml"><a href="#6-4配置文件yarn-site-xml" class="headerlink" title="6.4配置文件yarn-site.xml"></a>6.4配置文件yarn-site.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">&lt;!-- reducer获取数据的方式 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop103&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="6-5配置文件yarn-env-sh"><a href="#6-5配置文件yarn-env-sh" class="headerlink" title="6.5配置文件yarn-env.sh"></a>6.5配置文件yarn-env.sh</h3><p>在其中修改java的路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br></pre></td></tr></table></figure><h3 id="6-6配置hadoop-env-sh"><a href="#6-6配置hadoop-env-sh" class="headerlink" title="6.6配置hadoop-env.sh"></a>6.6配置hadoop-env.sh</h3><p><strong>hadoop-env.sh 由于Hadoop是java进程，所以需要添加jdk</strong></p><p>修改hadoop-env.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br></pre></td></tr></table></figure><h3 id="6-7配置文件mapred-site-xml"><a href="#6-7配置文件mapred-site-xml" class="headerlink" title="6.7配置文件mapred-site.xml"></a>6.7配置文件mapred-site.<strong>xml</strong></h3><p>先改名，因为本身是没有mapred-site.xml这个文件的</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p>改名完成后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]#vim mapred-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定mr运行在yarn上 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>这些配置文件改好后，返回/opt/module目录下</p><p>把hadoop102下修改的文件分发到hadoop103和hadoop104下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 module]# scp -r hadoop root@hadoop103:/opt/module/</span><br><span class="line">[root@hadoop102 module]# scp -r hadoop root@hadoop104:/opt/module/</span><br></pre></td></tr></table></figure><h2 id="7-测试集群"><a href="#7-测试集群" class="headerlink" title="7.测试集群"></a>7.测试集群</h2><p>在完成配置文件等一系列工作后，我们要开始测试集群了</p><p>先格式化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop2 hadoop-2.7.2]# bin/hdfs namenode –format</span><br></pre></td></tr></table></figure><p>之后启动hdfs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop2 hadoop-2.7.2]# sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[root@hadoop2 hadoop-2.7.2]# jps</span><br><span class="line">4166 NameNode</span><br><span class="line">4482 Jps</span><br><span class="line">4263 DataNode</span><br><span class="line"></span><br><span class="line">[root@hadoop3 桌面]# jps</span><br><span class="line">3218 DataNode</span><br><span class="line">3288 Jps</span><br><span class="line"></span><br><span class="line">[root@hadoop4 桌面]# jps</span><br><span class="line">3221 DataNode</span><br><span class="line">3283 SecondaryNameNode</span><br><span class="line">3364 Jps</span><br></pre></td></tr></table></figure><p>如果是这样这表示启动hdfs成功</p><p>下面启动yarn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>启动完成后</p><p>在浏览器上访问可视化页面：<a href="http://192.168.128.172:50070" target="_blank" rel="noopener">http://192.168.128.172:50070</a></p><p><img src="http://qqa.nghqqa.cn/blog/20190811/CFi7BSr2G8Ws.png?imageslim" alt="mark"></p><p>到此为止，hadoop配置就结束了，谢谢大家的阅读。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;我们之前搭建了hadoop的伪分布式的集群，伪分布式的集群对于学习一些基础的hadoop操作是足够的，但是当你涉及到更复杂的操作时，伪分布式
      
    
    </summary>
    
      <category term="搭建自己的hadoop学习集群" scheme="www.nghqqa.cn/categories/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hadoop%E5%AD%A6%E4%B9%A0%E9%9B%86%E7%BE%A4/"/>
    
    
      <category term="hadoop" scheme="www.nghqqa.cn/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Linux学习</title>
    <link href="www.nghqqa.cn/2019/08/06/Linux%E5%AD%A6%E4%B9%A0/"/>
    <id>www.nghqqa.cn/2019/08/06/Linux学习/</id>
    <published>2019-08-06T11:28:07.000Z</published>
    <updated>2019-08-06T11:37:21.155Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>对于之前写的hadoop伪分布式的安装文章，可能对一些没有Linux基础的读者来说有一些的难度，所以建议大家先了解Linux的一些基础知识在来看之前的文章，可能就会觉得很简单了</p><h2 id="学习建议"><a href="#学习建议" class="headerlink" title="学习建议"></a>学习建议</h2><p>我个人是比较推荐大家可以去b站看看韩顺平老师的Linux教程，可以在b站直接找到，这里放出b站链接</p><p><a href="https://www.bilibili.com/video/av21303002?from=search&amp;seid=3872508779266125537" target="_blank" rel="noopener">https://www.bilibili.com/video/av21303002?from=search&amp;seid=3872508779266125537</a></p><p>视频是18年上传的，但是知识是不会过时的，当然如果大家不想那么系统的了解Linux的话，可以去看看这个老哥的博客文章，这里放出链接</p><p><a href="https://blog.csdn.net/weixin_41710054/article/details/89081599#22_vivim_19" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41710054/article/details/89081599#22_vivim_19</a></p><p>这个文章写的比较的细，基本不知道的命令或者是快捷键都可以去文章中看看，自己看文章学习，可以比看视频省下不少时间，里面的命令可以基本满足正常操作Linux系统的要求。</p><p>好的，本次Linux的学习建议就到这里了，希望大家生活愉快。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;对于之前写的hadoop伪分布式的安装文章，可能对一些没有Linux基础的读者来说有一些的难度，所以建议大家先了解Linux的一些基础知识在
      
    
    </summary>
    
      <category term="自己学习Linux" scheme="www.nghqqa.cn/categories/%E8%87%AA%E5%B7%B1%E5%AD%A6%E4%B9%A0Linux/"/>
    
    
      <category term="Linux" scheme="www.nghqqa.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>hadoop伪分布式安装</title>
    <link href="www.nghqqa.cn/2019/07/25/hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/"/>
    <id>www.nghqqa.cn/2019/07/25/hadoop伪分布式安装/</id>
    <published>2019-07-25T05:38:40.000Z</published>
    <updated>2019-08-11T08:47:11.572Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文介绍的主要是Hadoop的伪分布式的搭建以及遇到的相关问题的解决，做一下记录，jdk的安装这里就不做太多的介绍了，相信大家根据网上的安装介绍很快就能安装成功。</p><p>hadoop集群分为3种模型</p><ol><li>单机模型：测试使用 </li><li>伪分布式模型：运行于单机 </li><li>完全分布式模型：适用于多台机器</li></ol><p>以下是使用的环境</p><table><thead><tr><th align="left">操作环境</th><th>主机名</th><th>IP地址</th><th>jdk</th><th align="center">hadoop版本</th><th></th></tr></thead><tbody><tr><td align="left">centos6.8</td><td>hadoop101</td><td>192.168.128.171</td><td>jdk1.8.0_191</td><td align="center">hadoop-2.7.2</td><td></td></tr><tr><td align="left"></td><td></td><td></td><td></td><td align="center"></td><td></td></tr></tbody></table><p>本文中使用的各种包，后续我会进行上传，以方便读者的使用</p><h1 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a>安装hadoop</h1><h2 id="hadoop上传与解压"><a href="#hadoop上传与解压" class="headerlink" title="hadoop上传与解压"></a>hadoop上传与解压</h2><p>​    当我们配置好自己的虚拟机后，可以自行在网上下载xftp和xshell，来对于自己的虚拟机进行远程上传文件和远程操作，这两款软件对于学生而言都是免费的，大家可以自行在网站上下载，速度可能会有点慢。</p><p>​    当我们下载好这两款软件后，就可以将hadoop的解压包上传至自己的虚拟机上去，我们将解压包上传至/opt/software中，开始解压hadoop，将hadoop解压至/opt/module/中，同时建议将java也解压至/opt/module/中，方便后面的管理。</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br></pre></td></tr></table></figure><p>具体配置</p><p>在/etc/profile的最后面加上，关于vi编辑器的用法可以自行百度一下，简单用法应该几分钟就能学会</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><p>在配置环境变量完成后，记得要进行让它生效</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>即可生效</p><p>可以输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure><p>如果成功则显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 2.7.2</span><br><span class="line">Subversion Unknown -r Unknown</span><br><span class="line">Compiled by root on 2017-05-22T10:49Z</span><br><span class="line">Compiled with protoc 2.5.0</span><br><span class="line">From source with checksum d0fda26633fa762bff87ec759ebe689c</span><br><span class="line">This command was run using /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-common-2.7.2.jar</span><br></pre></td></tr></table></figure><p>到这里hadoop就算是安装好了</p><h1 id="配置hadoop"><a href="#配置hadoop" class="headerlink" title="配置hadoop"></a>配置hadoop</h1><p>在伪分布式里我们只需要改三个配置文件core-site.xml和hdfs-site.xml还有hadoop-env.sh</p><p>这三个文件在hadoop目录下的etc/hadoop文件夹下</p><p><strong>core-site.xml文件包含了NameNode主机地址，监听端口等信息，对于这个伪分布式模型来说，我的主机地址为hadoo101，NameNode默认使用的端口为8020。</strong></p><p>修改core-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hadoop101:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">        &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>hdfs-site.xml用于配置/HDFS的相关属性，例如数据块的副本参数，数据块的副本对于伪分布式来说应该为1</strong></p><p>修改hdfs-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 指定HDFS副本的数量 --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p><strong>hadoop-env.sh 由于Hadoop是java进程，所以需要添加jdk</strong></p><p>修改hadoop-env.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_191</span><br></pre></td></tr></table></figure><p>对于伪分布式来说，改这三个配置文件够了。</p><p>在配置文件完成后，我们需要对hadoop进行初始化</p><p>在hadoop-2.7.2的目录下输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>如果初始化成功的话，一个和下图相似</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/9c98NJ18De35.png?imageslim" alt="mark"></p><p>到这里hadoop的配置就已经完成了</p><h1 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h1><p>在hadoop-2.7.2目录下输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure><p>启动dfs</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure><p>启动yarn节点</p><p>启动成功应该和下图相似</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/15Fa0e8qUDCR.png?imageslim" alt="mark"></p><p>到这里我们的集群就算是启动成功了</p><p>我们可以在web端查看HDFS文件系统</p><p><a href="http://192.168.128.171:50070/" target="_blank" rel="noopener">http://192.168.128.171:50070</a></p><p>192.168.128.171是我的ip地址，如果配置的不同，改一下即可</p><p>web端的hdfs文件系统如下图所示</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/fKgRPKMecBlX.png?imageslim" alt="mark"></p><h1 id="测试集群"><a href="#测试集群" class="headerlink" title="测试集群"></a>测试集群</h1><p>在HDFS文件系统上创建一个input文件夹</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input</span><br></pre></td></tr></table></figure><p>在web端应该可以看到</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/GK25hp1DN8GK.png?imageslim" alt="mark"></p><p>我们上传一个文件看看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put LICENSE.txt /input</span><br></pre></td></tr></table></figure><p>LICENSE.txt是hadoop自带的一个TXT文件</p><p>如果上传成功在web端应该可以看到</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/C4OldlfJiDzs.png?imageslim" alt="mark"></p><p>这样就是上传成功了</p><p>我们在HDFS上跑一下MapReduce程序</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /input/LICENSE.txt /output</span><br></pre></td></tr></table></figure><p>这里说明一下MapReduce要在启动yarn下运行</p><p>查看运行结果</p><p>在web端：</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/4wgLyU3syVSv.png?imageslim" alt="mark"></p><p>part-r-00000这个就是运行出来的结果</p><p>我们可以使用命令行查看结果也可以把这个文件下载到本地，这里我们使用命令行查看</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -cat /output/part-r-00000</span><br></pre></td></tr></table></figure><p>返回如下结果</p><p><img src="http://pv4lxcno2.bkt.clouddn.com/blog/20190725/FHhHjKyLhTk9.png?imageslim" alt="mark"></p><p>到这里基本可以了，我们的hadoop已经安装配置好了，可以进行下一步的学习了</p><p>关于MapReduce的WordCount程序详解可以看这个</p><p><a href="https://blog.csdn.net/gulu_gulu_jp/article/details/51298164/" target="_blank" rel="noopener">https://blog.csdn.net/gulu_gulu_jp/article/details/51298164/</a></p><p>本次伪分布的配置就到这里了，如果还有问题可以向我留言，谢谢阅读，下次的文章应该是完全分布式的hadoop的安装教程了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文介绍的主要是Hadoop的伪分布式的搭建以及遇到的相关问题的解决，做一下记录，jdk的安装这里就不做太多的介绍了，相信大家根据网上的安装
      
    
    </summary>
    
      <category term="搭建自己的hadoop学习集群" scheme="www.nghqqa.cn/categories/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84hadoop%E5%AD%A6%E4%B9%A0%E9%9B%86%E7%BE%A4/"/>
    
    
      <category term="hadoop" scheme="www.nghqqa.cn/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="www.nghqqa.cn/2019/07/22/hello-world/"/>
    <id>www.nghqqa.cn/2019/07/22/hello-world/</id>
    <published>2019-07-21T23:42:50.603Z</published>
    <updated>2019-07-25T13:27:10.775Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
